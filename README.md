# Uncertainty in Machine Learning Predictions
My bachelor thesis is about uncertainty quantification of Deep Neural Networks. This repository contains the complementary implementations. 

# Abstract of the Thesis
After a careful review of basic concepts for neural networks and for Bayesian neural networks, we reveal inconsistencies in the framework of Dropout as uncertainty quantification method. To gain insights into a machine learning algorithm, we introduce first and second order local consistency as a novel condition on stochastic processes. Then, we prove that second order local consistency with a machine learning algorithm that minimizes the mean squared error with L2 regularization uniquely determines the covariance function of a Gaussian process between training inputs and test inputs. Building on this, we show that the novel concept of local consistency provides uncertainty estimates. Finally, we underpin our argument with some illustrative experiments.

